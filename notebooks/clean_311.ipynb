{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08fbfd1a-c0ed-40be-8816-d4b8b74092d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s2/1r_lbpts7pvcd7p7h17w62tm0000gn/T/ipykernel_98699/1633889113.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Resolution Action Updated Date'] = pd.to_datetime(df['Resolution Action Updated Date'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"data/lift_complaints.csv\")\n",
    "\n",
    "# Ensure the datetime column is parsed properly\n",
    "df['Resolution Action Updated Date'] = pd.to_datetime(df['Resolution Action Updated Date'], errors='coerce')\n",
    "\n",
    "# Clean the building address column\n",
    "df['Incident Address'] = df['Incident Address'].astype(str).str.upper().str.strip()\n",
    "\n",
    "# Drop exact duplicates of same address + same timestamp\n",
    "df_deduped = df.drop_duplicates(subset=['Incident Address', 'Resolution Action Updated Date'])\n",
    "\n",
    "# Save the cleaned dataset with full details\n",
    "df_deduped.to_csv(\"deduped_complaints_with_details.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b5b15aa-8a24-4009-b30e-65be8114d3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 8563\n",
      "Filtered rows: 5459\n",
      "Duplicates removed: 3104\n"
     ]
    }
   ],
   "source": [
    "print(\"Original rows:\", len(df))\n",
    "print(\"Filtered rows:\", len(df_deduped))\n",
    "print(\"Duplicates removed:\", len(df) - len(df_deduped))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98e573c1-042d-46ab-8ee7-0624a40417d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Incident Address Resolution Action Updated Date  count\n",
      "695      1325 JEROME AVENUE                     2025-03-06     21\n",
      "3660       4215 PARK AVENUE                     2025-03-06     19\n",
      "2359   247 WEST  145 STREET                     2025-04-11     18\n",
      "3436   390 EAST  158 STREET                     2025-04-10     17\n",
      "3256    3560 WEBSTER AVENUE                     2025-06-02     16\n",
      "2408    2505 BEDFORD AVENUE                     2025-06-05     16\n",
      "1609  1965 LAFAYETTE AVENUE                     2025-01-06     15\n",
      "3469   3990 BRONX BOULEVARD                     2025-01-21     14\n",
      "1008    150 LEFFERTS AVENUE                     2025-04-09     13\n",
      "3253    3560 WEBSTER AVENUE                     2025-02-13     13\n"
     ]
    }
   ],
   "source": [
    "# Group by Incident Address + Timestamp\n",
    "dupes = df.groupby(['Incident Address', 'Resolution Action Updated Date']).size().reset_index(name='count')\n",
    "\n",
    "# Filter for entries with duplicate complaints\n",
    "dupes = dupes[dupes['count'] > 1]\n",
    "\n",
    "# See top duplicate groups\n",
    "print(dupes.sort_values(by='count', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b12d8d0c-4f48-4b50-beca-ae7f71113a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unique Key, Created Date, Closed Date, Agency, Agency Name, Complaint Type, Descriptor, Location Type, Incident Zip, Incident Address, Street Name, Cross Street 1, Cross Street 2, Intersection Street 1, Intersection Street 2, Address Type, City, Landmark, Facility Type, Status, Due Date, Resolution Description, Resolution Action Updated Date, Community Board, BBL, Borough, X Coordinate (State Plane), Y Coordinate (State Plane), Open Data Channel Type, Park Facility Name, Park Borough, Vehicle Type, Taxi Company Borough, Taxi Pick Up Location, Bridge Highway Name, Bridge Highway Direction, Road Ramp, Bridge Highway Segment, Latitude, Longitude, Location]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "# Choose a suspicious address + timestamp from above\n",
    "address = '764 EAST 152 STREET'\n",
    "timestamp = '2025-06-14 00:00:00'\n",
    "\n",
    "# Show all rows with this combo\n",
    "dupe_rows = df[(df['Incident Address'] == address) &\n",
    "               (df['Resolution Action Updated Date'] == pd.Timestamp(timestamp))]\n",
    "\n",
    "print(dupe_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60c83ee5-7319-43ff-816d-386f46c99a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full-row duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for full-row duplicates\n",
    "full_dupes = df[df.duplicated(subset=df.columns.tolist())]\n",
    "print(\"Full-row duplicates:\", len(full_dupes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ad84e85-d9f2-4575-b204-1de15b25f9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Matched 2941 out of 5459 complaints.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# Load files\n",
    "complaints_df = pd.read_csv(\"deduped_complaints_with_details.csv\")\n",
    "buildings_df = pd.read_csv(\"combined_output.csv\")\n",
    "\n",
    "# Step 1: Normalize addresses\n",
    "def normalize_address(addr):\n",
    "    addr = str(addr).upper()\n",
    "    addr = re.sub(r'\\bSTREET\\b', 'ST', addr)\n",
    "    addr = re.sub(r'\\bAVENUE\\b', 'AVE', addr)\n",
    "    addr = re.sub(r'\\bROAD\\b', 'RD', addr)\n",
    "    addr = re.sub(r'\\bBOULEVARD\\b', 'BLVD', addr)\n",
    "    addr = re.sub(r'\\s+', ' ', addr).strip()\n",
    "    return addr\n",
    "\n",
    "buildings_df['full_address_clean'] = buildings_df['full_address'].apply(normalize_address)\n",
    "complaints_df['address_clean'] = complaints_df['Incident Address'].apply(normalize_address)\n",
    "\n",
    "# Step 2: Fuzzy match\n",
    "address_lookup = buildings_df['full_address_clean'].tolist()\n",
    "\n",
    "def fuzzy_match_address(addr, choices, threshold=90):\n",
    "    match, score = process.extractOne(addr, choices, scorer=fuzz.token_sort_ratio)\n",
    "    return match if score >= threshold else None\n",
    "\n",
    "complaints_df['matched_address'] = complaints_df['address_clean'].apply(\n",
    "    lambda x: fuzzy_match_address(x, address_lookup)\n",
    ")\n",
    "\n",
    "# Step 3: Flag stabilized buildings\n",
    "complaints_df['is_stabilized'] = complaints_df['matched_address'].notnull()\n",
    "\n",
    "# Step 4: Save\n",
    "complaints_df.to_csv(\"deduped_elevator_complaints_with_stabilized_flag_fuzzy.csv\", index=False)\n",
    "print(f\"Done. Matched {complaints_df['is_stabilized'].sum()} out of {len(complaints_df)} complaints.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13e7e763-7d4d-4cc3-87e7-c9282eb25721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fast matching complete. Saved to 'elevator_complaints_with_stabilized_flag.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load files\n",
    "complaints_df = pd.read_csv(\"deduped_complaints_with_details.csv\")\n",
    "buildings_df = pd.read_csv(\"combined_output.csv\")\n",
    "\n",
    "# --------- 1. Clean and parse building addresses ----------\n",
    "def extract_range_parts(addr):\n",
    "    match = re.match(r'(\\d+)(?: TO (\\d+))?\\s+(.*)', addr)\n",
    "    if match:\n",
    "        start = int(match.group(1))\n",
    "        end = int(match.group(2)) if match.group(2) else start\n",
    "        street = match.group(3).strip()\n",
    "        return start, end, street\n",
    "    return None, None, None\n",
    "\n",
    "buildings_df['full_address'] = buildings_df['full_address'].str.upper().str.strip()\n",
    "buildings_df[['start_num', 'end_num', 'street']] = buildings_df['full_address'].apply(\n",
    "    lambda x: pd.Series(extract_range_parts(x))\n",
    ")\n",
    "\n",
    "# --------- 2. Clean and parse complaint addresses ----------\n",
    "def extract_number_street(addr):\n",
    "    match = re.match(r'(\\d+)\\s+(.*)', str(addr))\n",
    "    if match:\n",
    "        return int(match.group(1)), match.group(2).strip()\n",
    "    return None, None\n",
    "\n",
    "complaints_df['address_clean'] = complaints_df['Incident Address'].str.upper().str.strip()\n",
    "complaints_df[['address_num', 'address_street']] = complaints_df['address_clean'].apply(\n",
    "    lambda x: pd.Series(extract_number_street(x))\n",
    ")\n",
    "\n",
    "# --------- 3. Match within street groups ----------\n",
    "# Initialize match result\n",
    "complaints_df['is_stabilized'] = False\n",
    "\n",
    "# Only match rows with valid numbers and streets\n",
    "valid_complaints = complaints_df.dropna(subset=['address_num', 'address_street'])\n",
    "\n",
    "# Group buildings by street\n",
    "building_groups = buildings_df.groupby('street')\n",
    "\n",
    "# Match by street\n",
    "for street, group in valid_complaints.groupby('address_street'):\n",
    "    if street not in building_groups.groups:\n",
    "        continue\n",
    "    bldgs = buildings_df.loc[building_groups.groups[street]]\n",
    "    comp_idx = group.index\n",
    "    comp_nums = group['address_num']\n",
    "\n",
    "    # Build interval mask per building\n",
    "    for idx in comp_idx:\n",
    "        num = complaints_df.at[idx, 'address_num']\n",
    "        matches = bldgs[(bldgs['start_num'] <= num) & (bldgs['end_num'] >= num)]\n",
    "        if not matches.empty:\n",
    "            complaints_df.at[idx, 'is_stabilized'] = True\n",
    "\n",
    "# --------- 4. Save output ----------\n",
    "complaints_df.to_csv(\"deduped_quickmatch.csv\", index=False)\n",
    "print(\" Fast matching complete. Saved to 'elevator_complaints_with_stabilized_flag.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec669b7-0a8e-48f3-bd86-0793150c2f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de34e3e-97f3-4794-911a-ec0fb6a06b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYCHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cef52b1e-e1b7-46f4-beb2-c4a235811ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial join complete. Output saved to 'elevator_complaints_with_nycha_flag_spatial.csv'\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Step 1: Load elevator complaints (as GeoDataFrame)\n",
    "complaints_df = pd.read_csv(\"deduped_complaints_with_details.csv\")\n",
    "\n",
    "# Drop rows without valid lat/lon\n",
    "complaints_df = complaints_df.dropna(subset=[\"Latitude\", \"Longitude\"])\n",
    "\n",
    "# Create Point geometry from lat/lon\n",
    "complaints_gdf = gpd.GeoDataFrame(\n",
    "    complaints_df,\n",
    "    geometry=[Point(xy) for xy in zip(complaints_df[\"Longitude\"], complaints_df[\"Latitude\"])],\n",
    "    crs=\"EPSG:4326\"  # WGS84\n",
    ")\n",
    "\n",
    "# Step 2: Load NYCHA Developments GeoData\n",
    "nycha_df = pd.read_csv(\"data/NYCHA_Public_Housing_Developments.csv\")\n",
    "\n",
    "# Parse 'the_geom' column into shapely polygons (assuming WKT format)\n",
    "from shapely import wkt\n",
    "nycha_df['geometry'] = nycha_df['the_geom'].apply(wkt.loads)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "nycha_gdf = gpd.GeoDataFrame(nycha_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "# Step 3: Spatial Join (complaint inside NYCHA polygon)\n",
    "joined = gpd.sjoin(complaints_gdf, nycha_gdf, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Step 4: Flag NYCHA matches\n",
    "joined['is_nycha'] = joined['DEVELOPMEN'].notnull()\n",
    "\n",
    "# Step 5: Save output\n",
    "joined.to_csv(\"elevator_complaints_with_nycha_flag_spatial.csv\", index=False)\n",
    "print(\"Spatial join complete. Output saved to 'elevator_complaints_with_nycha_flag_spatial.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013c7a4-4c1f-401d-844a-79e7a09b9b60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
