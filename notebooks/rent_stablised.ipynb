{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90e322be-1980-4270-87f4-49e8bc786a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to 'reparsed_queens_stabilized_buildings.csv'\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Known suffixes\n",
    "street_suffixes = [\"ST\", \"STREET\", \"AVE\", \"AVENUE\", \"BLVD\", \"ROAD\", \"RD\", \"PL\", \"PLACE\", \"DR\", \"DRIVE\", \"CT\", \"COURT\", \"WAY\", \"LN\", \"LANE\", \"PKWY\", \"TERRACE\", \"TER\", \"BROADWAY\"]\n",
    "\n",
    "def parse_line(line):\n",
    "    tokens = line.strip().split()\n",
    "    if len(tokens) < 10:\n",
    "        return None\n",
    "\n",
    "    zip_code = tokens[0]\n",
    "    bldgno1 = tokens[1]\n",
    "\n",
    "    suffix_idx = -1\n",
    "    for i, token in enumerate(tokens[2:], start=2):\n",
    "        if token.upper() in street_suffixes:\n",
    "            suffix_idx = i\n",
    "            break\n",
    "    if suffix_idx == -1:\n",
    "        return None\n",
    "\n",
    "    street_name = \" \".join(tokens[2:suffix_idx])\n",
    "    suffix = tokens[suffix_idx]\n",
    "    rest = tokens[suffix_idx+1:]\n",
    "\n",
    "    if len(rest) < 7:\n",
    "        return None\n",
    "\n",
    "    block = rest[-2]\n",
    "    lot = rest[-1]\n",
    "    county, city, status1, status2, status3 = rest[-7:-2]\n",
    "\n",
    "    return [zip_code, bldgno1, street_name, suffix, county, city, status1, status2, status3, block, lot]\n",
    "\n",
    "# Parse the PDF\n",
    "pdf_path = \"2023-DHCR-Bldg-File-Queens.pdf\"\n",
    "parsed_data = []\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        lines = page.extract_text().split(\"\\n\")\n",
    "        for line in lines:\n",
    "            if line.startswith(\"ZIP\") or \"Source:\" in line:\n",
    "                continue\n",
    "            parsed = parse_line(line)\n",
    "            if parsed:\n",
    "                parsed_data.append(parsed)\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"ZIP\", \"BLDGNO1\", \"STREET1\", \"STSUFX1\", \"COUNTY\", \"CITY\", \"STATUS1\", \"STATUS2\", \"STATUS3\", \"BLOCK\", \"LOT\"]\n",
    "df = pd.DataFrame(parsed_data, columns=columns)\n",
    "\n",
    "# Build address\n",
    "df[\"full_address\"] = (\n",
    "    df[\"BLDGNO1\"].astype(str).str.strip() + \" \" +\n",
    "    df[\"STREET1\"].astype(str).str.strip() + \" \" +\n",
    "    df[\"STSUFX1\"].astype(str).str.strip()\n",
    ").str.upper()\n",
    "\n",
    "# Save result\n",
    "df.to_csv(\"reparsed_queens_stabilized_buildings.csv\", index=False)\n",
    "print(\"Saved to 'reparsed_queens_stabilized_buildings.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "069cc265-5827-4e3e-8c64-f5646f4354b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to 'staten_stabilized_buildings.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Known suffixes\n",
    "street_suffixes = [\"ST\", \"STREET\", \"AVE\", \"AVENUE\", \"BLVD\", \"ROAD\", \"RD\", \"PL\", \"PLACE\", \"DR\", \"DRIVE\", \"CT\", \"COURT\", \"WAY\", \"LN\", \"LANE\", \"PKWY\", \"TERRACE\", \"TER\", \"BROADWAY\"]\n",
    "\n",
    "def parse_line(line):\n",
    "    tokens = line.strip().split()\n",
    "    if len(tokens) < 10:\n",
    "        return None\n",
    "\n",
    "    zip_code = tokens[0]\n",
    "    bldgno1 = tokens[1]\n",
    "\n",
    "    suffix_idx = -1\n",
    "    for i, token in enumerate(tokens[2:], start=2):\n",
    "        if token.upper() in street_suffixes:\n",
    "            suffix_idx = i\n",
    "            break\n",
    "    if suffix_idx == -1:\n",
    "        return None\n",
    "\n",
    "    street_name = \" \".join(tokens[2:suffix_idx])\n",
    "    suffix = tokens[suffix_idx]\n",
    "    rest = tokens[suffix_idx+1:]\n",
    "\n",
    "    if len(rest) < 7:\n",
    "        return None\n",
    "\n",
    "    block = rest[-2]\n",
    "    lot = rest[-1]\n",
    "    county, city, status1, status2, status3 = rest[-7:-2]\n",
    "\n",
    "    return [zip_code, bldgno1, street_name, suffix, county, city, status1, status2, status3, block, lot]\n",
    "\n",
    "# Parse the PDF\n",
    "pdf_path = \"2023-DHCR-Bldg-File-Staten-Island.pdf\"\n",
    "parsed_data = []\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        lines = page.extract_text().split(\"\\n\")\n",
    "        for line in lines:\n",
    "            if line.startswith(\"ZIP\") or \"Source:\" in line:\n",
    "                continue\n",
    "            parsed = parse_line(line)\n",
    "            if parsed:\n",
    "                parsed_data.append(parsed)\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"ZIP\", \"BLDGNO1\", \"STREET1\", \"STSUFX1\", \"COUNTY\", \"CITY\", \"STATUS1\", \"STATUS2\", \"STATUS3\", \"BLOCK\", \"LOT\"]\n",
    "df = pd.DataFrame(parsed_data, columns=columns)\n",
    "\n",
    "# Build address\n",
    "df[\"full_address\"] = (\n",
    "    df[\"BLDGNO1\"].astype(str).str.strip() + \" \" +\n",
    "    df[\"STREET1\"].astype(str).str.strip() + \" \" +\n",
    "    df[\"STSUFX1\"].astype(str).str.strip()\n",
    ").str.upper()\n",
    "\n",
    "# Save result\n",
    "df.to_csv(\"reparsed_staten_stabilized_buildings.csv\", index=False)\n",
    "print(\"Saved to 'staten_stabilized_buildings.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e40f56ba-3955-4ffc-a2c1-a1c113198daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Matched 4822 out of 8558 complaints.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# Load files\n",
    "complaints_df = pd.read_csv(\"elevator_complaints_matched_to_nycha.csv\")\n",
    "buildings_df = pd.read_csv(\"combined_output.csv\")\n",
    "\n",
    "# Step 1: Normalize addresses\n",
    "def normalize_address(addr):\n",
    "    addr = str(addr).upper()\n",
    "    addr = re.sub(r'\\bSTREET\\b', 'ST', addr)\n",
    "    addr = re.sub(r'\\bAVENUE\\b', 'AVE', addr)\n",
    "    addr = re.sub(r'\\bROAD\\b', 'RD', addr)\n",
    "    addr = re.sub(r'\\bBOULEVARD\\b', 'BLVD', addr)\n",
    "    addr = re.sub(r'\\s+', ' ', addr).strip()\n",
    "    return addr\n",
    "\n",
    "buildings_df['full_address_clean'] = buildings_df['full_address'].apply(normalize_address)\n",
    "complaints_df['address_clean'] = complaints_df['Incident Address'].apply(normalize_address)\n",
    "\n",
    "# Step 2: Fuzzy match\n",
    "address_lookup = buildings_df['full_address_clean'].tolist()\n",
    "\n",
    "def fuzzy_match_address(addr, choices, threshold=90):\n",
    "    match, score = process.extractOne(addr, choices, scorer=fuzz.token_sort_ratio)\n",
    "    return match if score >= threshold else None\n",
    "\n",
    "complaints_df['matched_address'] = complaints_df['address_clean'].apply(\n",
    "    lambda x: fuzzy_match_address(x, address_lookup)\n",
    ")\n",
    "\n",
    "# Step 3: Flag stabilized buildings\n",
    "complaints_df['is_stabilized'] = complaints_df['matched_address'].notnull()\n",
    "\n",
    "# Step 4: Save\n",
    "complaints_df.to_csv(\"elevator_complaints_with_stabilized_flag_fuzzy.csv\", index=False)\n",
    "print(f\"Done. Matched {complaints_df['is_stabilized'].sum()} out of {len(complaints_df)} complaints.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "423ceada-6dc3-46df-84b1-45d542d05f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 5 files into 'combined_output.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Step 1: Define your folder path and file pattern\n",
    "folder_path = \"nyc_rent\"  # <-- change this to your folder\n",
    "file_pattern = os.path.join(folder_path, \"*.csv\")\n",
    "\n",
    "# Step 2: Use glob to get all CSV files in the folder\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "# Step 3: Read and concatenate all CSVs\n",
    "combined_df = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "\n",
    "# Step 4: Save to a single CSV\n",
    "combined_df.to_csv(\"combined_output.csv\", index=False)\n",
    "\n",
    "print(f\"Combined {len(csv_files)} files into 'combined_output.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a27c4b-7952-4a31-8bb8-ebb29836b869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
